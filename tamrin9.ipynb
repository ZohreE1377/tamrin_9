{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zohreh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zohreh\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "# Create an Elasticsearch client\n",
    "es = Elasticsearch(hosts=[\"http://localhost:9200\"])  # Replace \"localhost\" with your Elasticsearch server's address\n",
    "es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV data using pandas\n",
    "csv_file = \"C:/Users/zohreh/Desktop/books_info.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a list of dictionaries\n",
    "data = df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zohreh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create the index.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = 'http://localhost:9200'\n",
    "index_name = 'book_index'\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"url\": {\"type\": \"keyword\"},\n",
    "            \"contributors\": {\"type\": \"keyword\"},\n",
    "            \"date\": {\"type\": \"keyword\"},\n",
    "            \"format\": {\"type\": \"keyword\"},\n",
    "            \"full_text_url\": {\"type\": \"keyword\"},\n",
    "            \"trove_id\": {\"type\": \"keyword\"},\n",
    "            \"language\": {\"type\": \"keyword\"},\n",
    "            \"rights\": {\"type\": \"keyword\"},\n",
    "            \"pages\": {\"type\": \"integer\"},\n",
    "            \"form\": {\"type\": \"keyword\"},\n",
    "            \"volume\": {\"type\": \"keyword\"},\n",
    "            \"children\": {\"type\": \"keyword\"},\n",
    "            \"parent\": {\"type\": \"keyword\"},\n",
    "            \"text_downloaded\": {\"type\": \"keyword\"},\n",
    "            \"text_file\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.put(f'{endpoint}/{index_name}', json=index_mapping)\n",
    "if response.status_code == 200:\n",
    "    print(\"Index created successfully!\")\n",
    "else:\n",
    "    print(\"Failed to create the index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "\n",
    "es = Elasticsearch(hosts=['http://localhost:9200'])\n",
    "print(\"Elasticsearch is ready.\")\n",
    "cluster = Cluster(['localhost'])\n",
    "session = cluster.connect()\n",
    "print(\"Cassandra is ready.\\n\")\n",
    "\n",
    "#ElasticSearch settings:\n",
    "index_name = 'book_index'\n",
    "batch_size = 1000\n",
    "sort_field = '_doc'\n",
    "sort_order = 'asc'\n",
    "i=0\n",
    "search_after = None\n",
    "\n",
    "#Cassandra Settings:\n",
    "insert_query_for_parents = \"INSERT INTO book2.parents (trove_id,title,url,contributors,date,format,pages) VALUES (?,?,?,?,?,?,?)\"\n",
    "create_table_query_parents= \"CREATE TABLE book2.parents (\\\n",
    "    title text,\\\n",
    "    url text,\\\n",
    "    contributors text,\\\n",
    "    date text,\\\n",
    "    format text,\\\n",
    "    trove_id text PRIMARY KEY,\\\n",
    "    pages int\\\n",
    ");\"\n",
    "# Prepare the insert statement\n",
    "insert_statement = session.prepare(insert_query_for_parents)\n",
    "\n",
    "#STEP 1: Find parents and add them to goly.parents table:\n",
    "while True:\n",
    "    res = es.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            'query': {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"exists\": {\n",
    "                        \"field\": \"children\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"bool\": {\n",
    "                        \"must_not\": [\n",
    "                            {\n",
    "                                \"term\": {\n",
    "                                    \"children\": \"\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "            'sort': [{sort_field: sort_order}],\n",
    "            'size': batch_size,\n",
    "            'search_after': search_after\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if res['hits']['hits']:\n",
    "        last_hit = res['hits']['hits'][-1]\n",
    "        search_after = last_hit['sort']\n",
    "        \n",
    "        for hit in res['hits']['hits']:\n",
    "            doc_id = hit['_id']\n",
    "            doc = hit['_source']\n",
    "            i=i+1\n",
    "            # Perform any necessary data transformations or mappings here\n",
    "            row = (doc['trove_id'], doc['title'], doc['url'], doc['contributors'], doc['date'], doc['format'], int(doc['pages']))\n",
    "            session.execute(insert_statement, row)\n",
    "            print(f\"{i} docs added to book2.parents.\", end=\"\\r\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal parents documents:{i}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84348\n",
      "[{'index': {'_index': 'book_index', '_id': 'book_index_1'}}, {'title': 'Goliath Joe, fisherman / by Charles Thackeray (Wobbegong)', 'url': 'https://trove.nla.gov.au/work/10013347', 'contributors': 'Thackeray, Charles', 'date': '1900-1919', 'format': 'Book|Book/Illustrated', 'fulltext_url': 'https://nla.gov.au/nla.obj-2831231419', 'trove_id': 'nla.obj-2831231419', 'language': 'English', 'rights': 'Out of Copyright|http://rightsstatements.org/vocab/NKC/1.0/', 'pages': '130', 'form': 'Book', 'volume': '', 'parent': '', 'children': '', 'text_downloaded': 'True', 'text_file': 'goliath-joe-fisherman-by-charles-thackeray-wob-nla.obj-2831231419.txt'}, {'index': {'_index': 'book_index', '_id': 'book_index_2'}}, {'title': 'Grammar of the Narrinyeri tribe of Australian Aborigines / by the late Rev. G. Taplin', 'url': 'https://trove.nla.gov.au/work/10029401', 'contributors': 'Taplin, George', 'date': '1878-1880', 'format': 'Book|Government publication', 'fulltext_url': 'http://nla.gov.au/nla.obj-688657424', 'trove_id': 'nla.obj-688657424', 'language': 'English', 'rights': 'Out of Copyright|http://rightsstatements.org/vocab/NKC/1.0/', 'pages': '24', 'form': 'Book', 'volume': '', 'parent': '', 'children': '', 'text_downloaded': 'True', 'text_file': 'grammar-of-the-narrinyeri-tribe-of-australian-abor-nla.obj-688657424.txt'}, {'index': {'_index': 'book_index', '_id': 'book_index_3'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zohreh\\AppData\\Local\\Temp\\ipykernel_26924\\121720579.py:34: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use the 'operations' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es.bulk(body=my_body)\n"
     ]
    }
   ],
   "source": [
    "#step1\n",
    "import csv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def counter():\n",
    "    i = 0\n",
    "    def increment():\n",
    "        nonlocal i\n",
    "        i += 1\n",
    "        return i\n",
    "    return increment\n",
    "\n",
    "file=open(csv_file, 'r', encoding='utf-8')\n",
    "i = counter()\n",
    "\n",
    "reader = csv.DictReader(file)\n",
    "my_body = []\n",
    "for row in reader:\n",
    "    index_name = f\"book_index_{i()}\"\n",
    "    my_body.append({\n",
    "        \"index\": {\n",
    "            \"_index\": 'book_index',\n",
    "            \"_id\": index_name\n",
    "        }\n",
    "    })\n",
    "    my_body.append(dict(row))\n",
    "\n",
    "print(len(my_body))\n",
    "print(my_body[:5])\n",
    "\n",
    "\n",
    "\n",
    "# Call es.bulk() method to send data to Elasticsearch\n",
    "response = es.bulk(body=my_body)\n",
    "#csv_file.close()  # Close the CSV file after use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#children\n",
    "es = Elasticsearch(['http://localhost:9200'])  # Replace 'localhost' with your Elasticsearch cluster address\n",
    "\n",
    "# Connect to Cassandra\n",
    "cluster = Cluster(['localhost'])  # Replace 'localhost' with your Cassandra cluster address\n",
    "session = cluster.connect()\n",
    "\n",
    "# Define the keyspace and table\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS my_book WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}\")\n",
    "session.set_keyspace('my_book')\n",
    "children=session.execute(\"CREATE TABLE IF NOT EXISTS children_books (trove_id TEXT PRIMARY KEY, title TEXT, url TEXT, contributors TEXT, date TEXT, format TEXT, pages INT)\")\n",
    "parents=session.execute(\"CREATE TABLE IF NOT EXISTS parents_books (trove_id TEXT PRIMARY KEY, title TEXT, url TEXT, contributors TEXT, date TEXT, format TEXT, pages INT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zohreh\\AppData\\Local\\Temp\\ipykernel_26924\\344606885.py:27: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  result = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total children documents: 1423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Retrieve data from Elasticsearch\n",
    "es_index = 'book_index'  # Replace 'book_index' with the actual Elasticsearch index name\n",
    "sort_field = '_doc'  # Replace 'your_sort_field' with the actual field to sort on\n",
    "sort_order = 'asc'  # Replace 'asc' with 'desc' if you want to sort in descending order\n",
    "batch_size = 1000  # Adjust the batch size as per your requirement\n",
    "search_after = None\n",
    "i = 0\n",
    "\n",
    "# Prepare insert statement for children_books table\n",
    "insert = session.prepare(\"INSERT INTO children_books (trove_id, title, url, contributors, date, format, pages) VALUES (?, ?, ?, ?, ?, ?, ?)\")\n",
    "\n",
    "# Retrieve and store data from Elasticsearch\n",
    "while True:\n",
    "    result = es.search(\n",
    "        index=es_index,\n",
    "        body={\n",
    "            'query': {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"exists\": {\n",
    "                                \"field\": \"parent\"\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"regexp\": {\n",
    "                                \"parent\": \".+\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'sort': [{sort_field: sort_order}],\n",
    "            'size': batch_size,\n",
    "            'search_after': search_after\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    if result['hits']['hits']:\n",
    "        last_hit = result['hits']['hits'][-1]\n",
    "        search_after = last_hit['sort']\n",
    "\n",
    "        for hit in result['hits']['hits']:\n",
    "            doc_id = hit['_id']\n",
    "            doc = hit['_source']\n",
    "            i += 1\n",
    "            # Perform any necessary data transformations or mappings here\n",
    "            date= doc['date']\n",
    "            if len(date) <= 4:\n",
    "                row = (doc['trove_id'], doc['title'], doc['url'], doc['contributors'], date, doc['format'], int(doc['pages']))\n",
    "            else:\n",
    "                row = (doc['trove_id'], doc['title'], doc['url'], doc['contributors'], date[5:], doc['format'], int(doc['pages']))\n",
    "            session.execute(insert, row)\n",
    "            \n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal children documents: {i}\\n\")\n",
    "\n",
    "# Close the connections\n",
    "session.shutdown()\n",
    "cluster.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parents\n",
    "from elasticsearch import Elasticsearch\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "\n",
    "es = Elasticsearch(hosts=['http://localhost:9200'])\n",
    "cluster = Cluster(['localhost'])\n",
    "session = cluster.connect()\n",
    "# Create the keyspace if not exists\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS my_book WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}\")\n",
    "session.set_keyspace('my_book')\n",
    "# Elasticsearch settings\n",
    "index_name = 'my_index3'\n",
    "batch_size = 1000\n",
    "sort_field = '_doc'\n",
    "sort_order = 'asc'\n",
    "i = 0\n",
    "search_after = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cassandra settings\n",
    "insert_query = \"INSERT INTO my_book.parents (trove_id, title, url, contributors, date, format, pages) VALUES (?, ?, ?, ?, ?, ?, ?)\"\n",
    "table_query_parents = \"CREATE TABLE IF NOT EXISTS my_book.parents (title text, url text, contributors text, date text, format text, trove_id text PRIMARY KEY, pages int);\"\n",
    "\n",
    "# Create the table in Cassandra\n",
    "session.execute(table_query_parents)\n",
    "\n",
    "# Prepare the insert statement\n",
    "insert_statement = session.prepare(insert_query)\n",
    "\n",
    "while True:\n",
    "    result = es.search(\n",
    "        index='book_index',\n",
    "        body={\n",
    "            'query': {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"exists\": {\n",
    "                                \"field\": \"children\"\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"bool\": {\n",
    "                                \"must_not\": [\n",
    "                                    {\n",
    "                                        \"term\": {\n",
    "                                            \"children\": \"\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'sort': [{sort_field: sort_order}],\n",
    "            'size': batch_size,\n",
    "            'search_after': search_after\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if result['hits']['hits']:\n",
    "        last_hit = result['hits']['hits'][-1]\n",
    "        search_after = last_hit['sort']\n",
    "        \n",
    "        for hit in result['hits']['hits']:\n",
    "            doc_id = hit['_id']\n",
    "            doc = hit['_source']\n",
    "            i=i+1\n",
    "            # Perform any necessary data transformations or mappings here\n",
    "            row = (doc['trove_id'], doc['title'], doc['url'], doc['contributors'], doc['date'], doc['format'], int(doc['pages']))\n",
    "            session.execute(insert_statement, row)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal parents documents:{i}\\n\")\n",
    "\n",
    "# Close the connections\n",
    "session.shutdown()\n",
    "cluster.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
